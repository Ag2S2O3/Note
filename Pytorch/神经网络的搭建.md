# 神经网络的搭建

## 神经网络的模板

```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
```

init是神经网络中的内容，forward是神经网络的处理过程

即，在 `__init__` 定义了一些处理函数，在训练过程中会不断调用 `forward` 执行其中的内容并进行计算

### 卷积层

所有元素相乘求和

```python
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

`in_channels` 输入图像的通道数
`out_channels` 输出图像的通道数
`kernel_size` 卷积核的大小  （它的值会在训练过程中不断调整）
`stride` 为步长，即卷积核移动的距离
`padding` 为填充，即图像周围需要扩展填充的像素

### 池化层

取其中的最大值

```python
torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```

`kernel_size` 池化核（跟卷积的差不多）
`stride` 步长，默认值是kernel_size的大小
`padding` 同上
`dilation` 空洞卷积，一般不做设置

### 非线性激活

依照一些公式改变数值（引入非线性特征）

查阅手册即可

### 正则化层

归一化

```python
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
```

看手册即可

### 线性层

对输入数据进行线性变换，从而改变数据的维度

>注意，可以使用 `torch.flatten(img)` 来进行展平（变成一行形式）

```python
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
```

$y=xA^T+b$

`bias` 表示是否需要进行偏置（+b）

### 随机失活

防止过拟合

```python
torch.nn.Dropout(p=0.5, inplace=False)
```

